{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize"
      ],
      "metadata": {
        "id": "cvX2FmZMDUaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "9nShBOfvqtxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google-Drive Mounting\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers\n",
        "!pip install transformers\n",
        "import transformers\n",
        "\n",
        "# Datasets\n",
        "!pip install datasets\n",
        "import datasets\n",
        "\n",
        "#NLTK\n",
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Sklearn (Performance Metric Calculation)\n",
        "import sklearn\n",
        "\n",
        "#Random for setting seeds\n",
        "import random\n",
        "\n",
        "#Install Google Translator\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "\n",
        "#Import pickle\n",
        "import pickle\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pJYo8SOUqtRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting"
      ],
      "metadata": {
        "id": "gfLfN-e-qKe8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfVxQxa3lPAE"
      },
      "source": [
        "# Google-Drive Mounting\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Anpassen der Working Directory\n",
        "os.chdir('drive/MyDrive')\n",
        "\n",
        "# Working directory\n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "tFwvPMWWDclS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read files"
      ],
      "metadata": {
        "id": "ZdCKW7q-qbbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read data\n",
        "def read_excel(excel_path, textcolumn = None, labelcolumn = None, maxrow = None):\n",
        "  #Read excel\n",
        "  df = pd.DataFrame(pd.read_excel(excel_path))\n",
        "  if textcolumn != None:\n",
        "    df = df[[textcolumn, labelcolumn]]\n",
        "    #Rename columns: Text in column 0, label in column 1 (necessary for model-training)\n",
        "    df.columns = ['text', 'labels']\n",
        "  #Limit data to maximum number of rows (for testing)\n",
        "  if maxrow != None:\n",
        "    df = df.iloc[:maxrow, :]\n",
        "  return df\n",
        "\n"
      ],
      "metadata": {
        "id": "4peLEHz7qeJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Training data\n",
        "training_nlp = read_excel('0_Data_General/04_RP_Unfalltyp2TRAIN.xlsx', 'Description', 'AccidentType')\n",
        "training_full = read_excel('0_Data_General/04_RP_Unfalltyp2TRAIN.xlsx')\n",
        "\n",
        "#Test data\n",
        "testing_nlp = read_excel('0_Data_General/04_RP_Unfalltyp2TEST.xlsx', 'Description', 'AccidentType')\n",
        "testing_full = read_excel('0_Data_General/04_RP_Unfalltyp2TEST.xlsx')\n",
        "\n",
        "#Validation data\n",
        "validation_nlp = read_excel('0_Data_General/04_RP_Unfalltyp2VALID.xlsx','Description', 'AccidentType')\n",
        "validation_full = read_excel('0_Data_General/04_RP_Unfalltyp2VALID.xlsx')"
      ],
      "metadata": {
        "id": "ZbDQSpl5rZFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#All possible turning 3ATs\n",
        "classes_list = [201, 202, 203, 204, 209,\n",
        "                211, 212, 213, 214, 215, 219,\n",
        "                221, 222, 223, 224, 225, 229,\n",
        "                231, 232, 233, 239,\n",
        "                241, 242, 243, 244, 245, 249,\n",
        "                251, 252, 259,\n",
        "                261, 262, 269,\n",
        "                271, 272, 273, 274, 275, 279,\n",
        "                281, 282, 283, 284, 285, 286, 289,\n",
        "                299]\n",
        "\n",
        "#Recode\n",
        "def recode(df, classes_list):\n",
        "  #Copy existing dataframe\n",
        "  df_new = df.copy()\n",
        "  #Set labels to zero according to classlist\n",
        "  df_new['labels'] = df_new.apply(lambda x: classes_list.index(x['labels']), axis = 1)\n",
        "  return df_new\n",
        "\n"
      ],
      "metadata": {
        "id": "ExahrG2js3e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let data start at zero\n",
        "train = recode(training_nlp, classes_list)\n",
        "\n",
        "test = recode(testing_nlp, classes_list)\n",
        "\n",
        "valid = recode(validation_nlp, classes_list)"
      ],
      "metadata": {
        "id": "Qfk-a9O8eA_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "qxEDCZq8qGKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Align / correct accident descriptions"
      ],
      "metadata": {
        "id": "wAf00fck08ue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEvZduTcpo4o"
      },
      "outputs": [],
      "source": [
        "#Input\n",
        "#Text | Label\n",
        "#Align accident descriptions\n",
        "def correct_participant(df):\n",
        "  df_new = df.copy()\n",
        "  #Replace certain words, but not case sensitive -> lower / upper cases do not matter\n",
        "  #Participant 1\n",
        "  df_new[\"text\"]= df_new[\"text\"].str.replace(\"ON 01\", \"Beteiligter 1\", case = False)\n",
        "  df_new[\"text\"]= df_new[\"text\"].str.replace(\"ON01\", \"Beteiligter 1\", case = False)\n",
        "  df_new[\"text\"]= df_new[\"text\"].str.replace(\"01\", \"Beteiligter 1\", case = False)\n",
        "  df_new[\"text\"]= df_new[\"text\"].str.replace(\"Teilnehmer 1\", \"Beteiligter 1\", case = False) #relevant for data augmentation\n",
        "  #Participant 2\n",
        "  df_new[\"text\"]= df_new[\"text\"].str.replace(\"ON 02\", \"Beteiligter 2\", case = False)\n",
        "  df_new[\"text\"]= df_new[\"text\"].str.replace(\"ON02\", \"Beteiligter 2\", case = False)\n",
        "  df_new[\"text\"]= df_new[\"text\"].str.replace(\"02\", \"Beteiligter 2\", case = False)\n",
        "  df_new[\"text\"]= df_new[\"text\"].str.replace(\"Teilnehmer 2\", \"Beteiligter 2\", case = False) #relevant for data augmentation\n",
        "  ##_x000D_\n",
        "  df_new[\"text\"] = df_new[\"text\"].str.replace(\"_x000D_\", \" \", case = False)\n",
        "  df_new[\"text\"] = df_new[\"text\"].str.replace(\"\\n\", \" \", case = False)\n",
        "  #Return\n",
        "  return df_new\n",
        "\n",
        "#Fill empty accident descriptions\n",
        "def correct_na(df):\n",
        "  df.text = df.text.fillna('Keine Unfallbeschreibung vorhanden.')\n",
        "  return df\n",
        "\n",
        "#Split into single paragraphs\n",
        "def split_paragraphs(data):\n",
        "  #Donwload nltk\n",
        "  nltk.download(\"punkt\")\n",
        "  #Split into list of sentences\n",
        "  data[\"text\"] = data.apply(lambda row: nltk.tokenize.sent_tokenize(row[\"text\"]), axis = 1)\n",
        "  return data\n",
        "\n",
        "\n",
        "#Transform to huggingface dataset\n",
        "def transform_dataset(data):\n",
        "  #Import classes\n",
        "  from datasets import Dataset, DatasetDict\n",
        "  #transform to pandas\n",
        "  dataset = Dataset.from_pandas(data)\n",
        "  #Return\n",
        "  return dataset\n",
        "\n",
        "\n",
        "#Correct language main function\n",
        "def main_correct(df):\n",
        "  #Change accident description\n",
        "  df_lang = correct_participant(df)\n",
        "  #Correct empty descriptions\n",
        "  df_na = correct_na(df_lang)\n",
        "  #Split into single paragraphs\n",
        "  #df_split = split_paragraphs(df_na)\n",
        "  #WTransform to dataset\n",
        "  df_dataset = transform_dataset(df_na)\n",
        "  #Rückgabe\n",
        "  return df_dataset\n",
        "\n",
        "#Create datasetdict\n",
        "def create_datasetdict(train, test = None, valid = None):\n",
        "  from datasets import DatasetDict\n",
        "  #Create DatasetDict\n",
        "  ds = DatasetDict()\n",
        "  #Conduct corrections\n",
        "  try:\n",
        "    train_c = main_correct(train)\n",
        "    ds[\"train\"] = train_c\n",
        "    print(\"training data included\")\n",
        "  except ValueError:\n",
        "    print(\"No training data inserted\")\n",
        "  try:\n",
        "    if test is None:\n",
        "      print(\"No test data inserted\")\n",
        "    else:\n",
        "      test_c = main_correct(test)\n",
        "      ds[\"test\"] = test_c\n",
        "      print(\"test data included\")\n",
        "  except ValueError:\n",
        "    print(\"No test data inserted\")\n",
        "  try:\n",
        "    if valid is None:\n",
        "      print(\"No validation data inserted\")\n",
        "    else:\n",
        "      valid_c = main_correct(valid)\n",
        "      ds[\"valid\"] = valid_c\n",
        "      print(\"validation data included\")\n",
        "  except ValueError:\n",
        "    print(\"No validation data inserted\")\n",
        "  #Return\n",
        "  return ds\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create final Dataset for model training\n",
        "\n",
        "dataset = create_datasetdict(train, test, valid)"
      ],
      "metadata": {
        "id": "oHz_KCRjZMoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Dataset (before tokenization starts)\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "abspath = os.path.abspath('0_Ergebnisse/220308_dataset.pkl')\n",
        "print(abspath)\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(dataset, handle, protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "JFhh9SJxF6eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "Easy Data Augmentation"
      ],
      "metadata": {
        "id": "fAibhlMXD2gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "#Function for translation to English of one sentence\n",
        "def translate_sent(sentence, language, timestop):\n",
        "  #Time break to not overload API\n",
        "  time.sleep(timestop)\n",
        "  #Create new Translator\n",
        "  translator = Translator()\n",
        "  #Translate\n",
        "  sentence_eng = translator.translate(sentence, dest = language).text\n",
        "  #return\n",
        "  return sentence_eng\n",
        "\n",
        "\n",
        "#Function to translate whole dataframe\n",
        "def main_translate(df, language, timestop):\n",
        "  #Activate tqdm\n",
        "  tqdm.pandas()\n",
        "  #Copy existing dataframe\n",
        "  df_new = df.copy()\n",
        "  #Translate\n",
        "  df_new[\"text\"] = df_new.progress_apply(lambda row: translate_sent(sentence = row[\"text\"], language = language, timestop = timestop), axis = 1)\n",
        "  #Return\n",
        "  return df_new\n"
      ],
      "metadata": {
        "id": "aEryLN7RLrHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Translate train data into English\n",
        "#Save translation\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "#Conduction correct participant function -> Replace ON01\n",
        "train_corrected = correct_participant(train)\n",
        "#Remove NAs\n",
        "train_corrected = correct_na(train_corrected)\n",
        "\n",
        "#Translate into English with 1 second break after each translation, works with 5 seconds\n",
        "train_en = main_translate(df = train_corrected, language = \"en\", timestop = 4)\n",
        "\n",
        "#Save translated data\n",
        "abspath = os.path.abspath('0_Ergebnisse/230422_train_en.pkl')\n",
        "print(abspath)\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(train_en, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "rn-CzSQpLrF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Translate data back from English to German\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "#File which should be translated back from English to German\n",
        "abspath = os.path.abspath('0_Ergebnisse/230422_train_en.pkl')\n",
        "\n",
        "# pkl einlesen. Colab unterstützt nur Pickle-Standard 4, während die Dateien aus Doc2Vec in Pickle-Standard 5 gespeichert wurden\n",
        "with open(str(abspath), 'rb') as pkl:\n",
        "  train_en = pickle.load(pkl)\n",
        "\n",
        "\n",
        "#Translate back from English to German with break after each translation, works with 5 seconds\n",
        "train_en_de = main_translate(df = train_en, language = \"de\", timestop = 4)\n",
        "\n",
        "\n",
        "#Save translated data\n",
        "abspath = os.path.abspath('0_Ergebnisse/230422_train_en_de.pkl')\n",
        "print(abspath)\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(train_en_de, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DHYCQCy4cE6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combination of single data with already augmented data\n",
        "\n",
        "#File with augmented data\n",
        "abspath = os.path.abspath('0_Ergebnisse/230422_train_en_de.pkl')\n",
        "\n",
        "# read ülö\n",
        "with open(str(abspath), 'rb') as pkl:\n",
        "  train_en_de = pickle.load(pkl)\n",
        "\n",
        "#Correct augmented data\n",
        "train_en_de_corrected = correct_participant(train_en_de)\n",
        "\n",
        "#Combinate original training data with augmented training data\n",
        "train_aug = pd.concat([train, train_en_de_corrected])"
      ],
      "metadata": {
        "id": "qXdpvVvNhceP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create final augmented Dataset for model training\n",
        "\n",
        "dataset_aug = create_datasetdict(train_aug, test, valid)"
      ],
      "metadata": {
        "id": "bz6fstdgnR70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save augmented dataset\n",
        "\n",
        "abspath = os.path.abspath('0_Ergebnisse/220425_dataset_aug.pkl')\n",
        "print(abspath)\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(dataset_aug, handle, protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "m8mySVKTnlCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import augmented dataset\n",
        "\n",
        "#File with augmentet dataset\n",
        "abspath = os.path.abspath('0_Ergebnisse/220425_dataset_aug.pkl')\n",
        "\n",
        "# pkl einlesen.\n",
        "with open(str(abspath), 'rb') as pkl:\n",
        "  dataset_aug = pickle.load(pkl)"
      ],
      "metadata": {
        "id": "vPGGaKWFzrdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "SBjlHOrliG6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer: Functions"
      ],
      "metadata": {
        "id": "BsK3PwD5iLwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Data\n",
        "#Import AutoTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "#Tokenize Function\n",
        "def tokenize_function(data):\n",
        "  tokens = tokenizer(data[\"text\"], truncation = True)\n",
        "  return tokens\n",
        "\n",
        "#Tokenize and set format: Takes Dataset as Input (Train and Test data)\n",
        "def main_tokenize(dataset, tokenizer):\n",
        "  #Initialize Tokenizer\n",
        "  tokenizer = tokenizer\n",
        "  #Explanation: https://huggingface.co/docs/transformers/preprocessing\n",
        "  #Tokenize\n",
        "  dataset = dataset.map(tokenize_function, batched = True)\n",
        "  #Set Format and return tensors\n",
        "  dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "  #Return\n",
        "  return dataset\n"
      ],
      "metadata": {
        "id": "AkXhjeQhjTHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize and preprocess data for model: Without augmentation\n",
        "#Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
        "\n",
        "#Dataset Bert-Base-German-Cased (BBGC)\n",
        "dataset_bbgc = main_tokenize(dataset, tokenizer)"
      ],
      "metadata": {
        "id": "gz0bx9q8Zhio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize and preprocess data for model: With augmentation\n",
        "#Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
        "\n",
        "#Dataset Bert-Base-German-Cased (BBGC) with augmentation\n",
        "dataset_aug_bbgc = main_tokenize(dataset_aug, tokenizer)"
      ],
      "metadata": {
        "id": "zUJmJVEon7aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save tokenized datasets: Without augmentation\n",
        "\n",
        "\n",
        "abspath = os.path.abspath('0_Ergebnisse/220308_dataset_bbgc_pure.pkl')\n",
        "print(abspath)\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(dataset_bbgc, handle, protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "KzFhUzugGFzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save tokenized datasets: With augmentation\n",
        "\n",
        "\n",
        "abspath = os.path.abspath('0_Ergebnisse/220425_dataset_aug_bbgc_pure.pkl')\n",
        "print(abspath)\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(dataset_aug_bbgc, handle, protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "hM8TP9ugoLFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import tokenized datasets\n",
        "\n",
        "#Tokenized dataset without data augmentation\n",
        "abspath = os.path.abspath('0_Ergebnisse/220308_dataset_bbgc_pure.pkl')\n",
        "\n",
        "# pkl einlesen.\n",
        "with open(str(abspath), 'rb') as pkl:\n",
        "  dataset_bbgc = pickle.load(pkl)\n",
        "\n",
        "\n",
        "\n",
        "#Tokenized dataset with data augmentation\n",
        "abspath = os.path.abspath('0_Ergebnisse/220425_dataset_aug_bbgc_pure.pkl')\n",
        "\n",
        "# pkl einlesen.\n",
        "with open(str(abspath), 'rb') as pkl:\n",
        "  dataset_aug_bbgc = pickle.load(pkl)"
      ],
      "metadata": {
        "id": "f-ZMO0vS0BOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling\n",
        "\n"
      ],
      "metadata": {
        "id": "cepXdf0CigGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: General functions"
      ],
      "metadata": {
        "id": "DU7EqyJ9isJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate Model Performance Metrics\n",
        "\n",
        "#Function for metric computation\n",
        "def compute_metrics(eval_pred):\n",
        "  #Sci-Kit-learn for evaluation\n",
        "  from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, f1_score\n",
        "  #Numpy\n",
        "  import numpy as np\n",
        "  #Get metrics\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis = -1)\n",
        "\n",
        "  accuracy = accuracy_score(y_true = labels, y_pred = predictions)\n",
        "  recall_micro = recall_score(y_true = labels, y_pred = predictions, average = \"micro\")\n",
        "  precision_micro = precision_score(y_true = labels, y_pred = predictions, average = \"micro\")\n",
        "  f1_micro = f1_score(y_true = labels, y_pred = predictions, average = \"micro\")\n",
        "\n",
        "  recall_macro = recall_score(y_true = labels, y_pred = predictions, average = \"macro\")\n",
        "  precision_macro = precision_score(y_true = labels, y_pred = predictions, average = \"macro\")\n",
        "  f1_macro = f1_score(y_true = labels, y_pred = predictions, average = \"macro\")\n",
        "\n",
        "  #return metric.compute(predictions = predictions, references = labels)\n",
        "  return {\"accuracy\": accuracy, \"precision_micro\": precision_micro, \"recall_micro\": recall_micro, \"f1_micro\": f1_micro,  \"precision_macro\": precision_macro, \"recall_macro\": recall_macro, \"f1_macro\": f1_macro}\n"
      ],
      "metadata": {
        "id": "gQwnel4civgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: Finetuning BBGC with Data without Augmentation (BBGCPURE)"
      ],
      "metadata": {
        "id": "0ZwpEyvhEQSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reference: https://huggingface.co/course/chapter3/3?fw=pt\n",
        "#Data Collator, which concatenates the single tensors within a batch\n",
        "#Finetuning model: https://huggingface.co/course/chapter3/2?fw=pt\n",
        "#Trainer class: https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "\n",
        "from transformers import Trainer, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments\n",
        "\n",
        "#Set Seed\n",
        "random.seed(10)\n",
        "\n",
        "#Initialize Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
        "#Dynamic padding using data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "#Specify training arguments: First Argument is path were model is stored (limitation also ver \"max_steps = X\" possible )\n",
        "training_args = TrainingArguments(output_dir = \"bertbasegermancased_pure\", overwrite_output_dir = False,  evaluation_strategy = \"steps\", num_train_epochs = 20, load_best_model_at_end = True)\n"
      ],
      "metadata": {
        "id": "G-zJsdlvBzpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelconfiguration:\n",
        "#Bert-Base-German-Cased | Pure training data without any augmentation, just the basic language correction | 20 epochs\n",
        "\n",
        "#Determine model\n",
        "model_bbgc_puredata = AutoModelForSequenceClassification.from_pretrained('bert-base-german-cased', num_labels=len(classes_list))\n",
        "#Specify trainer\n",
        "trainer_bbgc_puredata = Trainer(\n",
        "    model_bbgc_puredata,\n",
        "    training_args,\n",
        "    train_dataset= dataset_bbgc[\"train\"],\n",
        "    eval_dataset= dataset_bbgc[\"test\"],\n",
        "    data_collator= data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "dPs0aqeyST69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute training\n",
        "trainer_bbgc_puredata.train()"
      ],
      "metadata": {
        "id": "XHNPtc0dblem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate model on test data set\n",
        "preds_results, preds_labels, preds_metrics = trainer_bbgc_puredata.predict(dataset_bbgc[\"test\"])  #All metrics and predictions\n",
        "\n",
        "print(preds_metrics)"
      ],
      "metadata": {
        "id": "nPhJuvSGfZaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: Finetuning BBGC with Data with Augmentation (BBGCAUG)"
      ],
      "metadata": {
        "id": "bMJhBY3vqAaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reference: https://huggingface.co/course/chapter3/3?fw=pt\n",
        "#Data Collator, which concatenates the single tensors within a batch\n",
        "#Finetuning model: https://huggingface.co/course/chapter3/2?fw=pt\n",
        "#Trainer class: https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "\n",
        "from transformers import Trainer, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments\n",
        "\n",
        "#Set Seed\n",
        "random.seed(10)\n",
        "\n",
        "#Initialize Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
        "#Dynamic padding using data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "#Specify training arguments: First Argument is path were model is stored (limitation also ver \"max_steps = X\" possible )\n",
        "training_args = TrainingArguments(output_dir = \"bertbasegermancased_aug\", overwrite_output_dir = False,  evaluation_strategy = \"steps\", num_train_epochs = 20, load_best_model_at_end = True)\n"
      ],
      "metadata": {
        "id": "ta3ldagRVX51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelconfiguration:\n",
        "#Bert-Base-German-Cased | training data with data augmentation, data was translated to english and back again and then just appended to the original training data | 20 epochs\n",
        "\n",
        "#Determine model\n",
        "model_bbgc_augdata = AutoModelForSequenceClassification.from_pretrained('bert-base-german-cased', num_labels=len(classes_list))\n",
        "#Specify trainer\n",
        "trainer_bbgc_augdata = Trainer(\n",
        "    model_bbgc_augdata,\n",
        "    training_args,\n",
        "    train_dataset= dataset_aug_bbgc[\"train\"],\n",
        "    eval_dataset= dataset_aug_bbgc[\"test\"],\n",
        "    data_collator= data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "0J2nhYpHqKeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute training\n",
        "trainer_bbgc_augdata.train()"
      ],
      "metadata": {
        "id": "_UA04KmjqkM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model BBGCAUG:\n",
        "#Loading best model from bertbasegermancased_pure/checkpoint-3500 (score: 0.6726800799369812).\n",
        "#TrainOutput(global_step=26000, training_loss=0.30083916932000565, metrics={'train_runtime': 6944.3512, 'train_samples_per_second': 29.941, 'train_steps_per_second': 3.744, 'total_flos': 2.552274605904876e+16, 'train_loss': 0.30083916932000565, 'epoch': 20.0})\n",
        "\n",
        "#Evaluate model on test data set\n",
        "preds_results, preds_labels, preds_metrics = trainer_bbgc_augdata.predict(dataset_aug_bbgc[\"test\"])  #Alle Metriken und alle Vorhersagen\n",
        "\n",
        "\n",
        "print(preds_metrics)"
      ],
      "metadata": {
        "id": "ktlSTcYrqpKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "tQLWXBBRFZxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer: BBGCPure with Finetuning and without Data augmentation\n",
        "\n",
        "BertBaseGermanCased Model trained with data without additional data augmentation"
      ],
      "metadata": {
        "id": "pGMM7Bgvz9x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import necessary classes\n",
        "from transformers import Trainer, AutoModelForSequenceClassification, TrainingArguments, AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "#Set Seed\n",
        "random.seed(10)\n",
        "\n",
        "#Load pretrained and finetuned model from storage:\n",
        "#path_bbgcpure = \"bertbasegermancase/checkpoint-2000\"\n",
        "path_bbgcpure = \"bertbasegermancased_pure/checkpoint-1500\"\n",
        "\n",
        "#Initialize loaded model\n",
        "model_bbgcpure = AutoModelForSequenceClassification.from_pretrained(path_bbgcpure, num_labels=len(classes_list), output_hidden_states = False) #HiddenStatesOutput = True für sentence embeddings\n",
        "\n",
        "#Set model in evaluation mode: Just forward propagation\n",
        "#model_bbgcpure.eval() #activate for embeddings / vectors\n",
        "\n",
        "#Initialize Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
        "#Dynamic padding using data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "\n",
        "#Define prediction trainer\n",
        "trainer_bbgcpure = Trainer(model_bbgcpure)\n",
        "\n",
        "\n",
        "trainer_bbgcpure = Trainer(\n",
        "    model_bbgcpure,\n",
        "    data_collator= data_collator,\n",
        "    compute_metrics=compute_metrics, #activate for embeddings / vectors\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "F5y2gVggvV5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer: BBGCAug with Finetuning and with Data augmentation\n",
        "\n",
        "BertBaseGermanCased Model trained with data with additional data augmentation"
      ],
      "metadata": {
        "id": "kDpwS4PS0l81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import necessary classes\n",
        "from transformers import Trainer, AutoModelForSequenceClassification, TrainingArguments, AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "#Set Seed\n",
        "random.seed(10)\n",
        "\n",
        "#Load pretrained and finetuned model from storage:\n",
        "#path_bbgcpure = \"bertbasegermancase/checkpoint-2000\"\n",
        "path_bbgc_augdata = \"bertbasegermancased_aug/checkpoint-3500\"\n",
        "\n",
        "#Initialize loaded model\n",
        "model_bbgc_augdata = AutoModelForSequenceClassification.from_pretrained(path_bbgc_augdata, num_labels=len(classes_list), output_hidden_states = False) #output_hidden_states = True für sentence embeddings\n",
        "\n",
        "#Set model in evaluation mode: Just forward propagation\n",
        "#model_bbgc_augdata.eval() #activate for sentence embeddings\n",
        "\n",
        "#Initialize Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased')\n",
        "#Dynamic padding using data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "\n",
        "#Define prediction trainer\n",
        "trainer_bbgc_augdata = Trainer(model_bbgc_augdata)\n",
        "\n",
        "\n",
        "trainer_bbgc_augdata = Trainer(\n",
        "    model_bbgc_augdata,\n",
        "    data_collator= data_collator,\n",
        "    compute_metrics=compute_metrics, #activate for vectors\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UrZLwU400lh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction: General functions"
      ],
      "metadata": {
        "id": "eGOJL0fE0pDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for predictions\n",
        "\n",
        "#Slice dataset into several batches, since otherwise the RAM is not enough.\n",
        "#https://huggingface.co/docs/datasets/v1.1.1/package_reference/main_classes.html#datasets.Dataset.shard\n",
        "\n",
        "\n",
        "from torch import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def flatten_list(listtobeflatten):\n",
        "  return [item for sublist in listtobeflatten for item in sublist ]\n",
        "\n",
        "\n",
        "\n",
        "def make_prediction(dataset, numberofbatches, trainer, startlayer, endlayer):\n",
        "  #Startlayer should be the second layer\n",
        "  #Endlayer should be the last layer (index starts at 0, bert-model has 12 layers + 1 input embedding layer, which is the first layer)\n",
        "  #Recommended values are therefore: startlayer = 1, endlayer = 12\n",
        "\n",
        "  datalist = []\n",
        "\n",
        "\n",
        "  for shard in range(0, numberofbatches, 1):\n",
        "    #Divide dataset into batches to save RAM\n",
        "    data = dataset.shard(numberofbatches, shard, contiguous = True)\n",
        "\n",
        "    #Make prediction\n",
        "    with torch.no_grad():\n",
        "      preds_overview = trainer.predict(data)\n",
        "\n",
        "    #Get predictions, which have the format of the model output: #https://huggingface.co/docs/transformers/main_classes/output\n",
        "    #predictions = tuple with 2 entries\n",
        "    #Length = Sum of accident cases, every entry has length of 47, which corresponds to number of 3ATs.\n",
        "    predictions = preds_overview[0]\n",
        "\n",
        "    #Get hidden states\n",
        "    #Hidden states are the second entrie of the prediction tuple: https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/bert#transformers.models.bert.modeling_bert.BertForPreTrainingOutput.hidden_states\n",
        "    #Hidden states are a tuple with length 13: 1 input embeddings + 12 bert specific layer\n",
        "    hidden_states = predictions[1]\n",
        "\n",
        "    #Convert to tensors\n",
        "    #Current dimensions: layers, batches, tokens, features\n",
        "    tensors = tuple(map(lambda layer: torch.from_numpy(layer), hidden_states))\n",
        "\n",
        "    #Combine the different layers to make one whole big tensor\n",
        "    token_embeddings = torch.stack(tensors, dim = 0)\n",
        "\n",
        "    #Reorder the embeddings to: accidents(batch) / layer / tokens / features\n",
        "    token_embeddings = token_embeddings.permute(1,0,2,3)\n",
        "\n",
        "    #Get / calculate sentence embeddings\n",
        "    sentence_embeddings = []\n",
        "\n",
        "    for batch in range(len(token_embeddings)):\n",
        "\n",
        "      #Select single accident\n",
        "       #Accident hat form: torch.Size([13, 467, 768])\n",
        "      accident = token_embeddings[batch]\n",
        "\n",
        "      #Select start to end layers\n",
        "      accidentstarttoendlayer = accident[startlayer:endlayer]\n",
        "\n",
        "      #Average layers\n",
        "      #Average layers: [467, 768]\n",
        "      layers = torch.mean(accidentstarttoendlayer, dim = 0)\n",
        "\n",
        "      #Average tokens to get sentence embeddings\n",
        "      #Average tokens: [768]\n",
        "      tokens = torch.mean(layers, dim = 0)\n",
        "\n",
        "      #Append to list\n",
        "      #Create list with length of batch size and one tensor with 768 dimensions.\n",
        "      sentence_embeddings.append(tokens)\n",
        "\n",
        "    #Append to list\n",
        "    datalist.append(sentence_embeddings)\n",
        "\n",
        "    #Flatten list\n",
        "    result = flatten_list(datalist)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g36qrBlLNX_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction: BBGCPure with Finetuning and without data augmentation"
      ],
      "metadata": {
        "id": "jpNSzXYj0ysD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#perform prediction\n",
        "#Test data\n",
        "#Prediction with Bert internal classifier\n",
        "\n",
        "#Prediction:\n",
        "v7_preds_results_test, v7_preds_labels_test, v7_preds_metrics_test = trainer_bbgcpure.predict(dataset_bbgc[\"test\"])"
      ],
      "metadata": {
        "id": "lcfKXL_T6DKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform prediction\n",
        "#Validation Data\n",
        "#Prediction with Bert internal classifier\n",
        "\n",
        "#Prediction:\n",
        "v7_preds_results, v7_preds_labels, v7_preds_metrics = trainer_bbgcpure.predict(dataset_bbgc[\"valid\"])"
      ],
      "metadata": {
        "id": "fShlzSMrkTy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Results\n",
        "#Test data\n",
        "#Prediction:\n",
        "import numpy as np\n",
        "\n",
        "print(v7_preds_metrics_test)\n",
        "\n",
        "v7_results_test = pd.DataFrame(dataset_bbgc[\"test\"][\"labels\"])\n",
        "v7_results_test.loc[:, \"Pred_test\"] = pd.DataFrame(np.argmax(v7_preds_results_test, axis = -1))\n",
        "v7_results_test.columns = [\"Y_test\", \"Pred_test\"]\n",
        "v7_results_test"
      ],
      "metadata": {
        "id": "DrxBishR6TV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save prediction results\n",
        "\n",
        "v7_results_test.to_pickle('0_Ergebnisse/Bert/results/V7_220703_test_preds_0.81')\n"
      ],
      "metadata": {
        "id": "2XsCCJNd6ohb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Results\n",
        "#Validation data\n",
        "#Prediction:\n",
        "import numpy as np\n",
        "\n",
        "print(v7_preds_metrics)\n",
        "\n",
        "v7_results = pd.DataFrame(dataset_bbgc[\"valid\"][\"labels\"])\n",
        "v7_results.loc[:, \"Pred_valid\"] = pd.DataFrame(np.argmax(v7_preds_results, axis = -1))\n",
        "v7_results.columns = [\"Y_valid\", \"Pred_valid\"]\n",
        "v7_results\n"
      ],
      "metadata": {
        "id": "V8SiLuNrll0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save prediction results\n",
        "\n",
        "v7_results.to_pickle('0_Ergebnisse/Bert/results/V7_220702_valid_preds_0.80')\n"
      ],
      "metadata": {
        "id": "wzX2nCFHne6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform prediction\n",
        "#test = dataset_bbgc[\"valid\"].select(range(0,100,1))\n",
        "\n",
        "#Get sentence embeddings for validation dataset with bbgcpure\n",
        "#Configuration: Dataset, 100 batches, Trainer, Startlayer = second layer, EndLayer = final layer\n",
        "sent_embed_valid_bbgcpure = make_prediction(dataset_bbgc[\"valid\"], 100, trainer_bbgcpure, 1, 12)\n",
        "sent_embed_train_bbgcpure = make_prediction(dataset_bbgc[\"train\"], 100, trainer_bbgcpure, 1, 12)\n",
        "sent_embed_test_bbgcpure = make_prediction(dataset_bbgc[\"test\"], 100, trainer_bbgcpure, 1, 12)\n",
        "\n",
        "#Check length\n",
        "#len(sent_embed_valid_bbgcpure)"
      ],
      "metadata": {
        "id": "FNDmgoiOUi5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save predicted sentence embeddings\n",
        "\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "#Wording: Datum_Versuchsnummer_Modell_Augmenation_Datensatz_Typ.pkl\n",
        "abspath_valid_bbgcpure = os.path.abspath('0_Ergebnisse/220402_0_bbgc_pure_valid_embeddings.pkl')\n",
        "abspath_train_bbgcpure = os.path.abspath('0_Ergebnisse/220402_0_bbgc_pure_train_embeddings.pkl')\n",
        "abspath_test_bbgcpure = os.path.abspath('0_Ergebnisse/220402_0_bbgc_pure_test_embeddings.pkl')\n",
        "\n",
        "\n",
        "with open(str(abspath_valid_bbgcpure), 'wb') as handle:\n",
        "  pickle.dump(sent_embed_valid_bbgcpure, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(str(abspath_train_bbgcpure), 'wb') as handle:\n",
        "  pickle.dump(sent_embed_train_bbgcpure, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(str(abspath_test_bbgcpure), 'wb') as handle:\n",
        "  pickle.dump(sent_embed_test_bbgcpure, handle, protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "fpEMUBIxWTLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction: BBGCAug with Finetuning and with augmentation data"
      ],
      "metadata": {
        "id": "kqoahcHy1oGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction with Bert internal classifier\n",
        "#Test data\n",
        "\n",
        "v8_preds_results_test, v8_preds_labels_test, v8_preds_metrics_test = trainer_bbgc_augdata.predict(dataset_aug_bbgc[\"test\"])"
      ],
      "metadata": {
        "id": "IZ33wmR562nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Results\n",
        "#Test data\n",
        "#Prediction:\n",
        "import numpy as np\n",
        "\n",
        "print(v8_preds_metrics_test)\n",
        "\n",
        "v8_results_test = pd.DataFrame(dataset_bbgc[\"test\"][\"labels\"])\n",
        "v8_results_test.loc[:, \"Pred_test\"] = pd.DataFrame(np.argmax(v8_preds_results_test, axis = -1))\n",
        "v8_results_test.columns = [\"Y_test\", \"Pred_test\"]\n",
        "v8_results_test"
      ],
      "metadata": {
        "id": "ER8crue67Be8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save prediction results\n",
        "\n",
        "v8_results_test.to_pickle('0_Ergebnisse/Bert/results/V8_220703_test_preds_0.80')"
      ],
      "metadata": {
        "id": "an7NdDXL7UQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction with Bert internal classifier\n",
        "#Validation data\n",
        "\n",
        "v8_preds_results, v8_preds_labels, v8_preds_metrics = trainer_bbgc_augdata.predict(dataset_aug_bbgc[\"valid\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "DSijxx7B14KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Results\n",
        "#Prediction:\n",
        "import numpy as np\n",
        "\n",
        "print(v8_preds_metrics)\n",
        "\n",
        "v8_results = pd.DataFrame(dataset_bbgc[\"valid\"][\"labels\"])\n",
        "v8_results.loc[:, \"Pred_valid\"] = pd.DataFrame(np.argmax(v8_preds_results, axis = -1))\n",
        "v8_results.columns = [\"Y_valid\", \"Pred_valid\"]\n",
        "v8_results\n"
      ],
      "metadata": {
        "id": "Ly1VBEqouWpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save prediction results\n",
        "\n",
        "v8_results.to_pickle('0_Ergebnisse/Bert/results/V8_220702_valid_preds_0.79')"
      ],
      "metadata": {
        "id": "z3TpX0S4uh8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get sentence embeddings for validation dataset with bbgcaug\n",
        "#Configuration: Dataset, 100 batches, Trainer, Startlayer = second layer, EndLayer = final layer\n",
        "sent_embed_valid_bbgcaug = make_prediction(dataset_aug_bbgc[\"valid\"], 100, trainer_bbgc_augdata, 1, 12)\n",
        "sent_embed_train_bbgcaug = make_prediction(dataset_aug_bbgc[\"train\"], 100, trainer_bbgc_augdata, 1, 12)\n",
        "sent_embed_test_bbgcaug = make_prediction(dataset_aug_bbgc[\"test\"], 100, trainer_bbgc_augdata, 1, 12)\n"
      ],
      "metadata": {
        "id": "rccX-VbB2R23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save predicted sentence embeddings\n",
        "\n",
        "#Wording: Datum_Versuchsnummer_Modell_Augmenation_Datensatz_Typ.pkl\n",
        "abspath_valid_bbgcaug = os.path.abspath('0_Ergebnisse/220426_0_bbgc_aug_valid_embeddings.pkl')\n",
        "abspath_train_bbgcaug = os.path.abspath('0_Ergebnisse/220426_0_bbgc_aug_train_embeddings.pkl')\n",
        "abspath_test_bbgcaug = os.path.abspath('0_Ergebnisse/220426_0_bbgc_aug_test_embeddings.pkl')\n",
        "\n",
        "\n",
        "with open(str(abspath_valid_bbgcaug), 'wb') as handle:\n",
        "  pickle.dump(sent_embed_valid_bbgcaug, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(str(abspath_train_bbgcaug), 'wb') as handle:\n",
        "  pickle.dump(sent_embed_train_bbgcaug, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(str(abspath_test_bbgcaug), 'wb') as handle:\n",
        "  pickle.dump(sent_embed_test_bbgcaug, handle, protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "VGouJoga3vXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Postprocessing"
      ],
      "metadata": {
        "id": "xcVwuVTTyVPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Postprocessing: General Functions"
      ],
      "metadata": {
        "id": "SaKff0XG1En5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions to process the predicted data and to combine it with the original dataframe\n",
        "\n",
        "from torch import take\n",
        "import pandas as pd\n",
        "\n",
        "#Convert sentence embeddings to pandas dataframe\n",
        "def embeddings_to_df(embeddings):\n",
        "\n",
        "  #Create empty list\n",
        "  list_of_df = []\n",
        "\n",
        "  for tensor in range(len(embeddings)):\n",
        "\n",
        "    #Convert to numpy arrays\n",
        "    arrays = embeddings[tensor].numpy()\n",
        "\n",
        "    #Convert to dataframe\n",
        "    df = pd.DataFrame(arrays)\n",
        "\n",
        "    #Transpose dataframe\n",
        "    df.t = df.transpose()\n",
        "\n",
        "    #Append to list\n",
        "    list_of_df.append(df.t)\n",
        "\n",
        "  #Concatenate list of dataframes to one big dataframe\n",
        "  #https://pandas.pydata.org/docs/reference/api/pandas.concat.html\n",
        "  df.con = pd.concat(list_of_df, ignore_index = True )\n",
        "\n",
        "  #Return\n",
        "  return df.con\n",
        "\n",
        "#Merge sentence embeddings dataframe with original dataset\n",
        "def merge_full_embeddings(dataset_full, sentence_embeddings):\n",
        "\n",
        "  df = pd.merge(dataset_full, sentence_embeddings, left_index = True, right_index = True)\n",
        "\n",
        "  return df\n",
        "\n",
        "#Main function for postprocessing\n",
        "def main_postprocess(dataset_full, embeddings):\n",
        "\n",
        "  #Convert embeddings to pandas dataframe\n",
        "  embed_pd = embeddings_to_df(embeddings)\n",
        "\n",
        "  #Merge full dataset with converted embeddings\n",
        "  df = merge_full_embeddings(dataset_full, embed_pd)\n",
        "\n",
        "  #Return\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p7PzJ3HqhCMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Postprocessing: BBGCPure with Finetuning and without data augmentation"
      ],
      "metadata": {
        "id": "OOVaRbLI1MYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do postprocessing: Combine dataframes\n",
        "\n",
        "validation_bbgcpure = main_postprocess(validation_full, sent_embed_valid_bbgcpure)\n",
        "training_bbgcpure = main_postprocess(training_full, sent_embed_train_bbgcpure)\n",
        "testing_bbgcpure = main_postprocess(testing_full, sent_embed_test_bbgcpure)\n",
        "\n",
        "#validation_bbgcpure"
      ],
      "metadata": {
        "id": "O7YxGL7xm2ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save final combined dataframe\n",
        "\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "#Wording: Datum_Versuchsnummer_Modell_Augmenation_Datensatz_Typ.pkl\n",
        "#Validation\n",
        "abspath = os.path.abspath('0_Ergebnisse/220402_0_bbgc_pure_valid_fulldataset.pkl')\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(validation_bbgcpure, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#Training\n",
        "abspath = os.path.abspath('0_Ergebnisse/220402_0_bbgc_pure_train_fulldataset.pkl')\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(training_bbgcpure, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "\n",
        "#Testing\n",
        "abspath = os.path.abspath('0_Ergebnisse/220402_0_bbgc_pure_test_fulldataset.pkl')\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(testing_bbgcpure, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save as excel\n",
        "\n",
        "validation_bbgcpure.to_excel('0_Ergebnisse/220402_0_bbgc_pure_valid_fulldataset.xlsx')\n",
        "\n",
        "training_bbgcpure.to_excel('0_Ergebnisse/220402_0_bbgc_pure_train_fulldataset.xlsx')\n",
        "\n",
        "testing_bbgcpure.to_excel('0_Ergebnisse/220402_0_bbgc_pure_test_fulldataset.xlsx')\n"
      ],
      "metadata": {
        "id": "kuwY1NcqpE1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Postprocessing: BBGCAug with Finetuning and with data augmentation"
      ],
      "metadata": {
        "id": "f_7YjRGM8blA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do the postprocessing\n",
        "\n",
        "validation_bbgcaug = main_postprocess(validation_full, sent_embed_valid_bbgcaug)\n",
        "training_bbgcaug = main_postprocess(training_full, sent_embed_train_bbgcaug)\n",
        "testing_bbgcaug = main_postprocess(testing_full, sent_embed_test_bbgcaug)\n",
        "\n",
        "validation_bbgcaug"
      ],
      "metadata": {
        "id": "TMdDeRfDAx6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save final combined data frames\n",
        "\n",
        "#Wording: Datum_Versuchsnummer_Modell_Augmenation_Datensatz_Typ.pkl\n",
        "#Validation\n",
        "abspath = os.path.abspath('0_Ergebnisse/220426_0_bbgc_aug_valid_fulldataset.pkl')\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(validation_bbgcaug, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#Training\n",
        "abspath = os.path.abspath('0_Ergebnisse/220426_0_bbgc_aug_train_fulldataset.pkl')\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(training_bbgcaug, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "\n",
        "#Testing\n",
        "abspath = os.path.abspath('0_Ergebnisse/220426_0_bbgc_aug_test_fulldataset.pkl')\n",
        "\n",
        "with open(str(abspath), 'wb') as handle:\n",
        "  pickle.dump(testing_bbgcaug, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save as excel\n",
        "\n",
        "validation_bbgcaug.to_excel('0_Ergebnisse/220426_0_bbgc_aug_valid_fulldataset.xlsx')\n",
        "\n",
        "training_bbgcaug.to_excel('0_Ergebnisse/220426_0_bbgc_aug_train_fulldataset.xlsx')\n",
        "\n",
        "testing_bbgcaug.to_excel('0_Ergebnisse/220426_0_bbgc_aug_test_fulldataset.xlsx')"
      ],
      "metadata": {
        "id": "jw4gIcwSDLdV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}